{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 크롤링/스크레이핑 실전과 데이터 활용 \n",
    "\n",
    "- 데이터 세트 추출과 활용\n",
    "- API로 데이터 수집하고 활용\n",
    "- 시계열 데이터 수집하고 활용\n",
    "- 열린 데이터 수집과 활용\n",
    "- 웹 페이지 자동 조작\n",
    "- 자바스크립트를 이용한 페이지 스크레이핑\n",
    "- 추출한 데이터 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "\n",
    "def make_dir(dir_name):\n",
    "    ''' 디렉토리 생성 '''\n",
    "    \n",
    "    if not exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "        msg = \"{} 디렉토리를 생성하였습니다!\".format(dir_name)\n",
    "    else:\n",
    "        msg = \"{} 디렉토리가 이미 존재합니다!\".format(dir_name)\n",
    "        \n",
    "    return msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 세트 추출과 활용 \n",
    "- Index of /kowiki/\n",
    "\n",
    "- 위키백과 데이터 세트 다운로드\n",
    "- 위키백과 데이터 세트에서 문장 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/dataset 디렉토리를 생성하였습니다!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dir('data/dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\n",
      "        \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\n",
      "\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n",
      "<head>\n",
      "        <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"/>\n",
      "        <title>kowiki dump progress on 20181101</title>\n",
      "        <link rel=\"stylesheet\" type=\"text/css\" href=\"/dumps.css\" />\n",
      "        <style type=\"text/css\">\n",
      "                .siteinfo {\n",
      "                        text-align: center;\n",
      "                }\n",
      "                li {\n",
      "                        list-style-type: none;\n",
      "                        padding: 0.5em 1.5em 0.5em 1.5em;\n",
      "                        background: #fff;\n",
      "                        margin-bottom: 1em;\n",
      "                }\n",
      "                li li {\n",
      "                        background-color: white;\n",
      "                        box-shadow: none;\n",
      "                        border-top: none;\n",
      "                        padding: 0px;\n",
      "                        margin-bottom: 0em;\n",
      "                }\n",
      "                li ul {\n",
      "                        margin-top: 4px;\n",
      "                        margin-bottom: 8px;\n",
      "                        box-shadow: none;\n",
      "                        border-top: none;\n",
      "                        padding: 0.5em 0px 0px;\n",
      "                }\n",
      "                .detail {\n",
      "                        font-weight: normal;\n",
      "                        font-style: italic;\n",
      "                }\n",
      "                .updates {\n",
      "                        font: monospace;\n",
      "                        font-size: smaller;\n",
      "                }\n",
      "                .status {\n",
      "                        font-weight: bold;\n",
      "                }\n",
      "                .status, .checksum {\n",
      "                        padding-left: 1em;\n",
      "                        padding-right: 1em;\n",
      "                }\n",
      "                li.in-progress {\n",
      "                        border-top: 3px solid #3366cc;\n",
      "                        font-weight: bold;\n",
      "                        box-shadow: 0 1px 1px rgba( 0, 0, 0, 0.15 );\n",
      "                }\n",
      "                li.done {\n",
      "                        border-top: 3px solid #00af89;\n",
      "                        box-shadow: 0 1px 1px rgba( 0, 0, 0, 0.15 );\n",
      "                }\n",
      "                li.failed {\n",
      "                        color: #b32424;\n",
      "                        font-weight: bold;\n",
      "                        border-top: 3px solid #b32424;\n",
      "                        box-shadow: 0 1px 1px rgba( 0, 0, 0, 0.15 );\n",
      "                }\n",
      "                .waiting {\n",
      "                        color: #c8ccd1;\n",
      "                        box-shadow: none;\n",
      "                        border-top: none;\n",
      "                }\n",
      "                .progress {\n",
      "                        font-family: monospace;\n",
      "                        font-size: 80%;\n",
      "                        margin-left: .5in;\n",
      "                }\n",
      "                .notice {\n",
      "                        border: none;\n",
      "                        background-color: transparent;\n",
      "                        font-weight: normal;\n",
      "                        color: #b32424;\n",
      "                }\n",
      "        </style>\n",
      "</head>\n",
      "\n",
      "<body>\n",
      "        <h1>kowiki dump progress on 20181101</h1>\n",
      "\n",
      "        <p class=\"siteinfo\">\n",
      "                This is the Wikimedia dump service.\n",
      "                Please read the <a href='../../legal.html'>copyrights</a> information.\n",
      "                See <a href=\"http://meta.wikimedia.org/wiki/Data_dumps\">Meta:Data dumps</a>\n",
      "                for documentation on the provided data formats.\n",
      "        </p>\n",
      "        <p class=\"notice\"></p>\n",
      "\n",
      "        <p>See <a href=\"../../backup-index.html\">all databases list</a>.</p>\n",
      "\n",
      "        <p class=\"previous\">\n",
      "                <a href=\"../20181020/\">Last dumped on 2018-10-20</a>\n",
      "        </p>\n",
      "\n",
      "        <p>\n",
      "          For a machine-readable version of the information on this page,\n",
      "          see <a href=\"dumpstatus.json\">the json status file.</a>\n",
      "        </p>\n",
      "\n",
      "        <p class=\"status\">\n",
      "                <span class='done'>Dump complete</span>\n",
      "        </p>\n",
      "\n",
      "        <p class=\"checksum\">\n",
      "                Verify downloaded files against the <a href=\"/kowiki/20181101/kowiki-20181101-md5sums.txt\">(md5)</a>, <a href=\"/kowiki/20181101/kowiki-20181101-sha1sums.txt\">(sha1)</a> checksums\n",
      "                to check for corrupted files.\n",
      "        </p>\n",
      "\n",
      "        <ul>\n",
      "                <li class='done'><span class='updates'>2018-11-02 16:02:35</span> <span class='status'>done</span> <span class='title'>Articles, templates, media/file descriptions, and primary meta-pages, in multiple bz2 streams, 100 pages per stream</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-pages-articles-multistream.xml.bz2\">kowiki-20181101-pages-articles-multistream.xml.bz2</a> 603.6 MB</li>\n",
      "<li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-pages-articles-multistream-index.txt.bz2\">kowiki-20181101-pages-articles-multistream-index.txt.bz2</a> 10.5 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-06 05:30:17</span> <span class='status'>done</span> <span class='title'>All pages with complete edit history (.7z)</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-pages-meta-history.xml.7z\">kowiki-20181101-pages-meta-history.xml.7z</a> 2.9 GB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-06 00:03:51</span> <span class='status'>done</span> <span class='title'>All pages with complete page edit history (.bz2)</span><div class='progress'>2018-11-06 00:03:49: kowiki (ID 11337) 1898462 pages (10.4|1784743.3/sec all|curr), 21523188 revs (118.5|176.7/sec all|curr), 98.8%|98.8% prefetched (all|curr), ETA 2018-11-06 03:20:33 [max 22921474]</div>\n",
      "<ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-pages-meta-history.xml.bz2\">kowiki-20181101-pages-meta-history.xml.bz2</a> 11.0 GB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-03 21:35:06</span> <span class='status'>done</span> <span class='title'>Log events to all pages and users.</span><ul><li class='detail'>This contains the log of actions performed on pages and users.</li>\n",
      "<li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-pages-logging.xml.gz\">kowiki-20181101-pages-logging.xml.gz</a> 58.6 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-03 00:39:38</span> <span class='status'>done</span> <span class='title'>All pages, current versions only.</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-pages-meta-current.xml.bz2\">kowiki-20181101-pages-meta-current.xml.bz2</a> 696.2 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-02 10:50:57</span> <span class='status'>done</span> <span class='title'><big><b>Articles, templates, media/file descriptions, and primary meta-pages.</b></big></span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-pages-articles.xml.bz2\">kowiki-20181101-pages-articles.xml.bz2</a> 556.1 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-01 20:02:49</span> <span class='status'>done</span> <span class='title'>First-pass for page XML data dumps</span><ul><li class='detail'>These files contain no page text, only revision metadata.</li>\n",
      "<li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-stub-meta-history.xml.gz\">kowiki-20181101-stub-meta-history.xml.gz</a> 1.4 GB</li>\n",
      "<li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-stub-meta-current.xml.gz\">kowiki-20181101-stub-meta-current.xml.gz</a> 145.2 MB</li>\n",
      "<li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-stub-articles.xml.gz\">kowiki-20181101-stub-articles.xml.gz</a> 104.3 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-03 21:23:04</span> <span class='status'>done</span> <span class='title'>Extracted page abstracts for Yahoo</span><div class='progress'>2018-11-03 21:22:56: kowiki (ID 7500) 2324 pages (91.7|563.9/sec all|curr), 2324 revs (91.7|78.6/sec all|curr), ETA 2018-11-04 04:17:31 [max 2282517]</div>\n",
      "<ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-abstract.xml.gz\">kowiki-20181101-abstract.xml.gz</a> 60.5 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-03 12:24:05</span> <span class='status'>done</span> <span class='title'>List of all page titles</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-all-titles.gz\">kowiki-20181101-all-titles.gz</a> 9.8 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-03 12:23:30</span> <span class='status'>done</span> <span class='title'>List of page titles in main namespace</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-all-titles-in-ns0.gz\">kowiki-20181101-all-titles-in-ns0.gz</a> 4.8 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-03 12:22:57</span> <span class='status'>done</span> <span class='title'>Namespaces, namespace aliases, magic words.</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-siteinfo-namespaces.json.gz\">kowiki-20181101-siteinfo-namespaces.json.gz</a> 6 KB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-02 01:51:07</span> <span class='status'>done</span> <span class='title'>Wiki page-to-page link records.</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-pagelinks.sql.gz\">kowiki-20181101-pagelinks.sql.gz</a> 204.3 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-02 01:49:17</span> <span class='status'>done</span> <span class='title'>List of pages' geographical coordinates</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-geo_tags.sql.gz\">kowiki-20181101-geo_tags.sql.gz</a> 1.0 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-02 01:49:05</span> <span class='status'>done</span> <span class='title'>Name/value pairs for pages.</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-page_props.sql.gz\">kowiki-20181101-page_props.sql.gz</a> 13.1 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-02 01:48:52</span> <span class='status'>done</span> <span class='title'>List of annotations (tags) for revisions and log entries</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-change_tag.sql.gz\">kowiki-20181101-change_tag.sql.gz</a> 20.9 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-02 01:48:32</span> <span class='status'>done</span> <span class='title'>Wiki category membership link records.</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-categorylinks.sql.gz\">kowiki-20181101-categorylinks.sql.gz</a> 67.6 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-02 01:48:00</span> <span class='status'>done</span> <span class='title'>Wiki external URL link records.</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-externallinks.sql.gz\">kowiki-20181101-externallinks.sql.gz</a> 58.5 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-02 01:47:39</span> <span class='status'>done</span> <span class='title'>Interwiki link tracking records</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-iwlinks.sql.gz\">kowiki-20181101-iwlinks.sql.gz</a> 5.8 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-02 01:47:29</span> <span class='status'>done</span> <span class='title'>Nonexistent pages that have been protected.</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-protected_titles.sql.gz\">kowiki-20181101-protected_titles.sql.gz</a> 3 KB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-02 01:47:15</span> <span class='status'>done</span> <span class='title'>Wiki template inclusion link records.</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-templatelinks.sql.gz\">kowiki-20181101-templatelinks.sql.gz</a> 49.3 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-02 01:46:16</span> <span class='status'>done</span> <span class='title'>Redirect list</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-redirect.sql.gz\">kowiki-20181101-redirect.sql.gz</a> 5.9 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-02 01:46:08</span> <span class='status'>done</span> <span class='title'>A few statistics such as the page count.</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-site_stats.sql.gz\">kowiki-20181101-site_stats.sql.gz</a> 821 bytes</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-02 01:45:55</span> <span class='status'>done</span> <span class='title'>User group assignments.</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-user_groups.sql.gz\">kowiki-20181101-user_groups.sql.gz</a> 4 KB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-02 01:45:47</span> <span class='status'>done</span> <span class='title'>This contains the SiteMatrix information from meta.wikimedia.org provided as a table.</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-sites.sql.gz\">kowiki-20181101-sites.sql.gz</a> 19 KB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-02 01:45:39</span> <span class='status'>done</span> <span class='title'>Wiki media/files usage records.</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-imagelinks.sql.gz\">kowiki-20181101-imagelinks.sql.gz</a> 16.4 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-02 01:45:27</span> <span class='status'>done</span> <span class='title'>Category information.</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-category.sql.gz\">kowiki-20181101-category.sql.gz</a> 2.7 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-02 01:45:14</span> <span class='status'>done</span> <span class='title'>Base per-page data (id, title, old restrictions, etc).</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-page.sql.gz\">kowiki-20181101-page.sql.gz</a> 60.6 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-02 01:44:59</span> <span class='status'>done</span> <span class='title'>Newer per-page restrictions table.</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-page_restrictions.sql.gz\">kowiki-20181101-page_restrictions.sql.gz</a> 35 KB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-02 01:28:35</span> <span class='status'>done</span> <span class='title'>Tracks which pages use which Wikidata items or properties and what aspect (e.g. item label) is used.</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-wbc_entity_usage.sql.gz\">kowiki-20181101-wbc_entity_usage.sql.gz</a> 11.8 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-02 01:28:19</span> <span class='status'>done</span> <span class='title'>Metadata on current versions of uploaded media/files.</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-image.sql.gz\">kowiki-20181101-image.sql.gz</a> 2.2 MB</li></ul></li>\n",
      "<li class='done'><span class='updates'>2018-11-02 01:27:53</span> <span class='status'>done</span> <span class='title'>Wiki interlanguage link records.</span><ul><li class='file'><a href=\"/kowiki/20181101/kowiki-20181101-langlinks.sql.gz\">kowiki-20181101-langlinks.sql.gz</a> 101.7 MB</li></ul></li>\n",
      "        </ul>\n",
      "\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "# ! wget https://dumps.wikimedia.org/kowiki/20181101/ -q -O -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2018-11-15 11:15:10--  https://dumps.wikimedia.org/kowiki/20181101/\n",
      "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.7\n",
      "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.7|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15049 (15K) [text/html]\n",
      "Saving to: './data/dataset/kowiki-2018110-index.html'\n",
      "\n",
      "     0K .......... ....                                       100% 2.10M=0.007s\n",
      "\n",
      "2018-11-15 11:15:11 (2.10 MB/s) - './data/dataset/kowiki-2018110-index.html' saved [15049/15049]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#! wget https://dumps.wikimedia.org/kowiki/20181101/ -O ./data/dataset/kowiki-2018110-index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>kowiki dump progress on 20181101</title>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "filepath = './data/dataset/kowiki-2018110-index.html'\n",
    "with open(filepath, encoding='utf-8') as fp:\n",
    "    soup = BeautifulSoup(fp, 'lxml')\n",
    "\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done_tags = soup.find_all('li', 'done')\n",
    "len(done_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li class=\"done\"><span class=\"updates\">2018-11-02 16:02:35</span> <span class=\"status\">done</span> <span class=\"title\">Articles, templates, media/file descriptions, and primary meta-pages, in multiple bz2 streams, 100 pages per stream</span><ul><li class=\"file\"><a href=\"/kowiki/20181101/kowiki-20181101-pages-articles-multistream.xml.bz2\">kowiki-20181101-pages-articles-multistream.xml.bz2</a> 603.6 MB</li>\n",
       "<li class=\"file\"><a href=\"/kowiki/20181101/kowiki-20181101-pages-articles-multistream-index.txt.bz2\">kowiki-20181101-pages-articles-multistream-index.txt.bz2</a> 10.5 MB</li></ul></li>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done_tag = done_tags[0]\n",
    "done_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-11-02 16:02:35'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updates = done_tag.find('span', 'updates').get_text()\n",
    "updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Articles, templates, media/file descriptions, and primary meta-pages, in multiple bz2 streams, 100 pages per stream'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = done_tag.find('span', 'title').get_text()\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'FileName': 'kowiki-20181101-pages-articles-multistream.xml.bz2',\n",
       "  'FileLink': '/kowiki/20181101/kowiki-20181101-pages-articles-multistream.xml.bz2'},\n",
       " {'FileName': 'kowiki-20181101-pages-articles-multistream-index.txt.bz2',\n",
       "  'FileLink': '/kowiki/20181101/kowiki-20181101-pages-articles-multistream-index.txt.bz2'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_tags = done_tag.find_all('li', 'file')\n",
    "files = []\n",
    "for file_tag in file_tags:\n",
    "    file_dict = {}\n",
    "    file_name = file_tag.find('a').get_text()\n",
    "    file_link = file_tag.find('a').get('href')\n",
    "    \n",
    "    file_dict['FileName'] = file_name\n",
    "    file_dict['FileLink'] = file_link\n",
    "    files.append(file_dict)\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def getKowikiInfos(filepath):\n",
    "\n",
    "    with open(filepath, encoding='utf-8') as fp:\n",
    "        soup = BeautifulSoup(fp, 'lxml')\n",
    "\n",
    "    Updates  = []\n",
    "    Title    = []\n",
    "    FileList = []   # 파일이름 리스트\n",
    "    Files    = []   # 파일상세 리스트\n",
    "\n",
    "    done_tags = soup.find_all('li', 'done')\n",
    "\n",
    "    for done_tag in done_tags:\n",
    "        # 등록날짜\n",
    "        updates = done_tag.find('span', 'updates').get_text()\n",
    "\n",
    "        # 제목\n",
    "        title = done_tag.find('span', 'title').get_text()\n",
    "\n",
    "        # 파일리스트\n",
    "        file_tags = done_tag.find_all('li', 'file')\n",
    "        files = []\n",
    "        filenames = []\n",
    "        for file_tag in file_tags:\n",
    "            file_dict = {}\n",
    "            file_name = file_tag.find('a').get_text()\n",
    "            file_link = file_tag.find('a').get('href')\n",
    "\n",
    "            file_dict['FileName'] = file_name\n",
    "            file_dict['FileLink'] = file_link\n",
    "            files.append(file_dict)\n",
    "\n",
    "            filenames.append(file_name)\n",
    "\n",
    "        Updates.append(updates)\n",
    "        Title.append(title)\n",
    "        FileList.append(tuple(filenames))\n",
    "        Files.append(files)\n",
    "\n",
    "    kokiwi_df = pd.DataFrame({ 'Updates'  : Updates, \n",
    "                               'Title'    : Title,\n",
    "                               'FileList' : FileList,\n",
    "                               'FileInfo' : Files }, \n",
    "                               columns = ['Updates', 'Title', 'FileList', 'FileInfo'])\n",
    "\n",
    "    return kokiwi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './data/dataset/kowiki-2018110-index.html'\n",
    "kokiwi_df = getKowikiInfos(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Updates</th>\n",
       "      <th>Title</th>\n",
       "      <th>FileList</th>\n",
       "      <th>FileInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-02 16:02:35</td>\n",
       "      <td>Articles, templates, media/file descriptions, ...</td>\n",
       "      <td>(kowiki-20181101-pages-articles-multistream.xm...</td>\n",
       "      <td>[{'FileName': 'kowiki-20181101-pages-articles-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-06 05:30:17</td>\n",
       "      <td>All pages with complete edit history (.7z)</td>\n",
       "      <td>(kowiki-20181101-pages-meta-history.xml.7z,)</td>\n",
       "      <td>[{'FileName': 'kowiki-20181101-pages-meta-hist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-11-06 00:03:51</td>\n",
       "      <td>All pages with complete page edit history (.bz2)</td>\n",
       "      <td>(kowiki-20181101-pages-meta-history.xml.bz2,)</td>\n",
       "      <td>[{'FileName': 'kowiki-20181101-pages-meta-hist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-11-03 21:35:06</td>\n",
       "      <td>Log events to all pages and users.</td>\n",
       "      <td>(kowiki-20181101-pages-logging.xml.gz,)</td>\n",
       "      <td>[{'FileName': 'kowiki-20181101-pages-logging.x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-11-03 00:39:38</td>\n",
       "      <td>All pages, current versions only.</td>\n",
       "      <td>(kowiki-20181101-pages-meta-current.xml.bz2,)</td>\n",
       "      <td>[{'FileName': 'kowiki-20181101-pages-meta-curr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Updates                                              Title  \\\n",
       "0  2018-11-02 16:02:35  Articles, templates, media/file descriptions, ...   \n",
       "1  2018-11-06 05:30:17         All pages with complete edit history (.7z)   \n",
       "2  2018-11-06 00:03:51   All pages with complete page edit history (.bz2)   \n",
       "3  2018-11-03 21:35:06                 Log events to all pages and users.   \n",
       "4  2018-11-03 00:39:38                  All pages, current versions only.   \n",
       "\n",
       "                                            FileList  \\\n",
       "0  (kowiki-20181101-pages-articles-multistream.xm...   \n",
       "1       (kowiki-20181101-pages-meta-history.xml.7z,)   \n",
       "2      (kowiki-20181101-pages-meta-history.xml.bz2,)   \n",
       "3            (kowiki-20181101-pages-logging.xml.gz,)   \n",
       "4      (kowiki-20181101-pages-meta-current.xml.bz2,)   \n",
       "\n",
       "                                            FileInfo  \n",
       "0  [{'FileName': 'kowiki-20181101-pages-articles-...  \n",
       "1  [{'FileName': 'kowiki-20181101-pages-meta-hist...  \n",
       "2  [{'FileName': 'kowiki-20181101-pages-meta-hist...  \n",
       "3  [{'FileName': 'kowiki-20181101-pages-logging.x...  \n",
       "4  [{'FileName': 'kowiki-20181101-pages-meta-curr...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kokiwi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('kowiki-20181101-pages-articles-multistream.xml.bz2',\n",
       " 'kowiki-20181101-pages-articles-multistream-index.txt.bz2')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kokiwi_df['FileList'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어 처리를 사용한 빈출 단어 추출 \n",
    "- 형태소 분석 : 자연어 처리의 기본 기술\n",
    "\n",
    "- 주어진 단어를 형태소라는 최소 단어로 분할해서 품사 등을 판별하는 것을 의미\n",
    "- 형태소 분석 라이브러리 : KoNLLPy, 파이썬 한국어 NLP\n",
    "- 한나눔\n",
    "- 꼬꼬마\n",
    "- Komoran\n",
    "- MeCab\n",
    "- 트위트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c5-01_konlpy_sample\n",
    "- KoNLPy로 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('아버지', 'NNG'),\n",
       " ('가방', 'NNG'),\n",
       " ('에', 'JKM'),\n",
       " ('들어가', 'VV'),\n",
       " ('시', 'EPH'),\n",
       " ('ㄴ다', 'EFN'),\n",
       " ('.', 'SF')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "\n",
    "kkma   = Kkma()\n",
    "malist = kkma.pos(\"아버지 가방에 들어가신다.\")\n",
    "malist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장에서 빈출 단어 추출 \n",
    "- c5-02_word_frequency \n",
    "- 위키백과 문장에서 빈출 단어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing word_frequency.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile  word_frequency.py\n",
    "# %%writefile  ./modules/word_frequency.py\n",
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    명령라인 매개변수로 지정한\n",
    "    디렉터리 내부의 파일을 읽어 들이고\n",
    "    빈출 단어를 출력합니다.\n",
    "    \"\"\"\n",
    "    # 명령어의 첫 번째 매개변수로\n",
    "    # WikiExtractor의 출력 디렉터리를 지정합니다.\n",
    "    input_dir = sys.argv[1]\n",
    "    print('input_dir :', input_dir)\n",
    "    kkma = Kkma()\n",
    "    \n",
    "    # 단어의 빈도를 저장하기 위한 Counter 객체를 생성합니다.\n",
    "    # Counter 클래스는 dict를 상속받는 클래스입니다.\n",
    "    frequency = Counter()\n",
    "    count_proccessed = 0\n",
    "    \n",
    "    # glob()으로 와일드카드 매치 파일 목록을 추출하고\n",
    "    # 매치한 모든 파일을 처리합니다.\n",
    "    # print('before for :', glob(os.path.join(input_dir, 'kowiki-*')))\n",
    "    # for path in glob(os.path.join(input_dir, '*', 'wiki_*')):\n",
    "    for path in glob(os.path.join(input_dir, 'kowiki-*')):\n",
    "        print('path :', path)\n",
    "        print('Processing {0}...'.format(path), file=sys.stderr)\n",
    "        \n",
    "        # 파일을 엽니다.\n",
    "        with open(path) as file:\n",
    "            # 파일 내부의 모든 기사에 반복을 돌립니다.\n",
    "            for content in iter_docs(file):\n",
    "                # 페이지에서 명사 리스트를 추출합니다.\n",
    "                tokens = get_tokens(kkma, content)\n",
    "                \n",
    "                # Counter의 update() 메서드로 리스트 등의 반복 가능 객체를 지정하면\n",
    "                # 리스트에 포함된 값의 출현 빈도를 세어줍니다.\n",
    "                frequency.update(tokens)\n",
    "                \n",
    "                # 10,000개의 글을 읽을 때마다 간단하게 출력합니다.\n",
    "                count_proccessed += 1\n",
    "                if count_proccessed % 10000 == 0:\n",
    "                    print('{0} documents were processed.'\n",
    "                        .format(count_proccessed),file=sys.stderr)\n",
    "    \n",
    "    # 모든 기사의 처리가 끝나면 상위 30개의 단어를 출력합니다\n",
    "    for token, count in frequency.most_common(30):\n",
    "        print(token, count)\n",
    "\n",
    "        \n",
    "def iter_docs(file):\n",
    "    \"\"\"\n",
    "    파일 객체를 읽어 들이고\n",
    "    기사의 내용(시작 태그 <doc>와 종료 태그 </doc> 사이의 텍스트)를 꺼내는\n",
    "    제너레이터 함수\n",
    "    \"\"\"\n",
    "    for line in file:\n",
    "        if line.startswith('<doc '):\n",
    "            # 시작 태그가 찾아지면 버퍼를 초기화합니다.\n",
    "            buffer = []\n",
    "        elif line.startswith('</doc>'):\n",
    "            # 종료 태그가 찾아지면 버퍼의 내용을 결합한 뒤 yield합니다.\n",
    "            content = ''.join(buffer)\n",
    "            yield content\n",
    "        else:\n",
    "            # 시작 태그/종료 태그 이외의 줄은 버퍼에 추가합니다.\n",
    "            buffer.append(line)\n",
    "\n",
    "            \n",
    "def get_tokens(kkma, content):\n",
    "    \"\"\"\n",
    "    문장 내부에 출현한 명사 리스트를 추출하는 함수\n",
    "    \"\"\"\n",
    "    # 명사를 저장할 리스트입니다.\n",
    "    tokens = []\n",
    "    node = kkma.pos(content)\n",
    "    for (taeso, pumsa) in node:\n",
    "        # 고유 명사와 일반 명사만 추출합니다.\n",
    "        if pumsa in ('NNG', 'NNP'):\n",
    "            tokens.append(taeso)\n",
    "    return tokens\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/result 디렉토리를 생성하였습니다!'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dir('./data/result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dir : .\\data\\dataset\n",
      "path : .\\data\\dataset\\kowiki-2018110-index.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing .\\data\\dataset\\kowiki-2018110-index.html...\n",
      "Traceback (most recent call last):\n",
      "  File \"word_frequency.py\", line 88, in <module>\n",
      "    main()\n",
      "  File \"word_frequency.py\", line 36, in main\n",
      "    for content in iter_docs(file):\n",
      "  File \"word_frequency.py\", line 71, in iter_docs\n",
      "    buffer.append(line)\n",
      "UnboundLocalError: local variable 'buffer' referenced before assignment\n"
     ]
    }
   ],
   "source": [
    "! python word_frequency.py .\\data\\dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API로 데이터 수집하고 활용 \n",
    "- 트위트에서 데이터 수집\n",
    "- 유튜브에서 데이터 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 트위터에서 데이터 수집 \n",
    "- 데이터 수집에 사용되는 2가지 API\n",
    "\n",
    "- REST API : \n",
    "- HTTP 요청을 전송하면 HTTP 응답을 반환하는 RESTful 형태로 트윗 또는 사용자 정보를 추출, \n",
    "- 호출 횟수 제한이 굉장히 엄격 \n",
    "- 한 사용자당 15분에 15회만 호출 제한이 있으므로, 대규모 데이터수집에는 좋지 않다. \n",
    "\n",
    "- Streaming API : \n",
    "- 한번의 요청을 보내면 서버와의 연결(컨넥션)을 유지하고, \n",
    "- 새로운 데이터가 추가될 때마다 서버가 데이터를 전송해주는 푸시 형태 \n",
    "- HTTP 요청을 보냈을때 연결을 확립한 상태로 두고 서버에서 메시지를 계속 전송받는 형태"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 트위터 API 인증 \n",
    "- OAuth 1.0a 인증 필요 (아래 4가지 모두 필요)\n",
    "\n",
    "- 애플리케니션 단위 발행 \n",
    "- Cousumer Key \n",
    "- Consuber Secret \n",
    "\n",
    "- 사용자 단위 발행 \n",
    "- Access Token \n",
    "- Access Token Secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 트위터 인증 정보를 위해 애플리케이션 등록\n",
    "\n",
    "- [ 애플리케이션 관리 화면 ] 에 트위터 계정으로 로그인\n",
    "- \"Create New App\" 버튼 클릭\n",
    "- 이름, 설명, 웹사이트 를 입력하고 사용약관 동의하면 애플리케이션이 생성된다.\n",
    "- \"Keys and Access Tokens\" 링크 클릭 후, 생성된 4개의 키 값을 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter Application Settings\n",
    "CONSUMER_KEY        = \"Rh9PPrGkDRohh5GualkQRErPu\"\n",
    "CONSUMER_SECRET     = \"Emg0dTXJXawpWn4EapiyG2aE5bWHGLreFd6vPy4XZMQmVb7ULp\"\n",
    "\n",
    "# Twitter Your Access Token\n",
    "ACCESS_TOKEN        = \"4477564098-AGJFdwERnNyEqVGHaPyy8yyloiPaCXsPob1q1Zy\"\n",
    "ACCESS_TOKEN_SECRET = \"otjvkl1U6POGZMw1SpGQHMHlyEPz9Z1TbhPIecFhCuI6e\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c5-03_rest_api_with_requests_oauthlib \n",
    "- Requests OAuthLib 를 이용한 타임라인 추출\n",
    "\n",
    "- 인증 정보를 사용해 OAuth1Session 객체를 생성\n",
    "- API 응답이 JSON 형식의 문자열이므로 response.json()으로 파싱\n",
    "- status는 트윗(Twitter API에서는 Status)를 나타내는 dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting oauth\n",
      "  Downloading https://files.pythonhosted.org/packages/e2/10/d7d6ae26ef7686109a10b3e88d345c4ec6686d07850f4ef7baefb7eb61e1/oauth-1.0.1.tar.gz\n",
      "Building wheels for collected packages: oauth\n",
      "  Running setup.py bdist_wheel for oauth: started\n",
      "  Running setup.py bdist_wheel for oauth: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Ente_LJS\\AppData\\Local\\pip\\Cache\\wheels\\f7\\a8\\ca\\272f26e0c3973e23fe6720ca1c98fb2f5630263ab11c90af62\n",
      "Successfully built oauth\n",
      "Installing collected packages: oauth\n",
      "Successfully installed oauth-1.0.1\n"
     ]
    }
   ],
   "source": [
    "! pip install oauth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ARneipia 앞으로 1일!! https://t.co/u7NEADETqQ\n",
      "@ARneipia RT @Pokemon_cojp: 11月17日（土）より和菓子をテーマにしたポケモンマスコットが登場！ 全部で6種類あって、どのポケモンかは、お楽しみだよ！ イーブイ茶房へ、ようこそ！ https://t.co/8YHXb9FOm3 #ポケモン https://t.co/wV…\n",
      "@ARneipia RT @hobby_magazine: 今年最大のヒットキャラクター!?『#ゲゲゲの鬼太郎』より、好評受注中の「#ねこ娘」製品サンプルを自然光フォト！\n",
      "https://t.co/04gKxlTTHf\n",
      "\n",
      "#鬼太郎 https://t.co/d2xmOgKxae\n",
      "@ARneipia RT @kitaroanime50th: 本日発売‼️\n",
      "#アニメージュ 12月号では #ゲゲゲの鬼太郎 描き下ろし表紙&amp;ピンナップポスターイラストを使用した缶バッジ6種セットの応募者全員サービスも実施中☝️初グッズ化のアニエス&amp;アデルにも注目です✨ https://t.co/c…\n",
      "@ARneipia 일도 바쁘고 참..뭐하다.. ㅇ 3 ㅇ...\n",
      "그냥 내 일이나 잘해야지.\n",
      "@ARneipia 소문이 돌아도 대응할 가치도 못느끼겠고..이젠 남에게 이건 잘못됬다고 말할 에너지조차 밑바닥이라 그냥 흘러가게 내버려두게된다. 내 목구멍이 급하지...\n",
      "@ARneipia https://t.co/Suv15VrxBP\n",
      "@ARneipia RT @DetPikachuMovie: Partner up with a legend. @VancityReynolds is #DetectivePikachu in theaters Summer 2019! https://t.co/QyX0TyUP74\n",
      "@ARneipia RT @Pokemon_cojp: アニメ「ポケットモンスター サン＆ムーン」で放送中の、前髪の長いイーブイのショートストーリー「イーブイどこいくの？」第4話を、期間限定で公開！ 続きは11月18日（日）の放送でチェックしよう！ #アニポケ https://t.co/VcnoK…\n",
      "@ARneipia ㅋㅋㅋ 허그또에 출연했닼ㅋㅋ 너무 미화해주셨닼ㅋㅋㅋ감사합니닼ㅋㅋㅋ 앜ㅋㅋㅋㅋ 기운내야지 ㅋㅋ..\n",
      "@ARneipia RT @GAMEFREAK_info: ＼RT＆フォローでGET！／\n",
      "\n",
      "RT＆フォローしてくれた方の中から抽選で、\n",
      "ディレクター増田順一とプロデューサー大森滋サイン入り！Switch同梱版 『ポケモン ピカ・ブイ』 どちらかを各1名様にプレゼント！\n",
      "\n",
      "#ゲームフリークCP\n",
      "@ARneipia 일이 많아 너무 힘들다.. 잘 할 수 있을까..정말 나 진짜 잘 하고 있는거 맞을까...머리가 복잡해\n",
      "@ARneipia RT @precure_movie: ❣️本編映像を解禁❣️\n",
      "\n",
      "大ヒット上映中の本作をちょっとお届け🎁\n",
      "歴代プリキュアのOPが、映画だけの特別メドレーでつながれたスペシャルなシーン😍\n",
      "\n",
      "ぜひ映画館の大迫力な画面・音楽で、５５人のプリキュアの魅力が詰まった本作をお楽しみください！…\n",
      "@ARneipia RT @aikatsu_dcd: データカードダス アイカツフレンズ！キセキのドレス編プロモーションビデオ ティザー編公開中！ロングバージョンは公式YouTubeチャンネル「アイチューブ」でご確認ください♪\n",
      "＃アイカツ　＃アイカツフレンズ https://t.co/45txTF…\n",
      "@ARneipia RT @amidakuji_DX: ピュアパレット復活！\n",
      "\n",
      "「アイカツフレンズ！」\n",
      "#aikatsu #aikatsufriends #アイカツフレンズ https://t.co/DQmw2Xb4nI\n",
      "@ARneipia RT @Pokemon_cojp: 発売目前！ 『ポケットモンスター Let's Go! ピカチュウ・Let's Go! イーブイ』の見どころを凝縮した新映像を公開！ さらに公式サイトでは、物語のクライマックスであるポケモンリーグや四天王の情報も！ https://t.co/T…\n",
      "@ARneipia RT @aikatsu_dcd: TVアニメ「アイカツフレンズ！」に登場する、あいねちゃんみおちゃんの大切なおまもりチャームがアイカツ！スタイルから発売するよ✨2つがそろうと、ひとつになるデザイン💖お店でも展示中！11月22日(木)から発売予定です✨ https://t.co/…\n",
      "@ARneipia RT @animage_tokuma: 【アニメージュ12月号／11月10日（土）発売】今年アニメ化50周年の『ゲゲゲの鬼太郎』が、創刊40周年のアニメージュで記念すべき初表紙！巻頭特集では今期『鬼太郎』を引っ張る魅力的なヒロインたちを大フィーチャーです！ #ゲゲゲの鬼太郎 h…\n",
      "@ARneipia RT @Pokemon_cojp: 『ポケットモンスター Let's Go! ピカチュウ・Let's Go! イーブイ』のTVCMが公開！ 『ポケモン GO』からポケモンを送ってもらえば、冒険はもっと楽しくなる！ 11月16日（金）発売！ ご予約受け付け中！ https://t…\n",
      "@ARneipia RT @amidakuji_DX: 「偶然、必然。」\n",
      "#aikatsu #aikatsufriends #アイカツフレンズ https://t.co/xMkrXr5u62\n"
     ]
    }
   ],
   "source": [
    "from requests_oauthlib import OAuth1Session\n",
    "import os\n",
    "\n",
    "# 환경변수에서 인증 정보를 추출합니다.\n",
    "# CONSUMER_KEY        = os.environ['CONSUMER_KEY']\n",
    "# CONSUMER_SECRET     = os.environ['CONSUMER_SECRET']\n",
    "# ACCESS_TOKEN        = os.environ['ACCESS_TOKEN']\n",
    "# ACCESS_TOKEN_SECRET = os.environ['ACCESS_TOKEN_SECRET']\n",
    "\n",
    "CONSUMER_KEY        = \"Rh9PPrGkDRohh5GualkQRErPu\"\n",
    "CONSUMER_SECRET     = \"Emg0dTXJXawpWn4EapiyG2aE5bWHGLreFd6vPy4XZMQmVb7ULp\"\n",
    "ACCESS_TOKEN        = \"4477564098-AGJFdwERnNyEqVGHaPyy8yyloiPaCXsPob1q1Zy\"\n",
    "ACCESS_TOKEN_SECRET = \"otjvkl1U6POGZMw1SpGQHMHlyEPz9Z1TbhPIecFhCuI6e\"\n",
    "\n",
    "\n",
    "# 인증 정보를 사용해 OAuth1Session 객체를 생성합니다.\n",
    "twitter = OAuth1Session(CONSUMER_KEY,\n",
    "                        client_secret=CONSUMER_SECRET,\n",
    "                        resource_owner_key=ACCESS_TOKEN,\n",
    "                        resource_owner_secret=ACCESS_TOKEN_SECRET)\n",
    "\n",
    "# 사용자의 타임라인을 추출합니다.\n",
    "response = twitter.get('https://api.twitter.com/1.1/statuses/home_timeline.json')\n",
    "\n",
    "# API 응답이 JSON 형식의 문자열이므로 response.json()으로 파싱합니다.\n",
    "# status는 트윗(Twitter API에서는 Status라고 부릅니다)를 나타내는 dict입니다.\n",
    "for status in response.json():\n",
    "    # 사용자 이름과 트윗을 출력합니다.\n",
    "    print('@' + status['user']['screen_name'], status['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c5-04_rest_api_with_tweepy \n",
    "- tweepy 를 이용한 타임라인 추출\n",
    "\n",
    "- 연결 확립한 상태로 메시지 계속 전송\n",
    "- 각 메시지는 줄바꿈 코드 CRLF로 구분\n",
    "- 메시지는 대부분 트윗을 나타내는 JSON 형식의 문자열\n",
    "- 트윗 이외의 연결을 유지하기 위한 공백과 메타 정보도 전송된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@alsa6865 RT @4CTR1CE5: 아 신이시여,,, ,,, 으악 ,,,,,, ㅠ 난 퀴리 &gt;부인&lt;이 아니야,,, ,,, ㅠㅠㅠ https://t.co/UG035Tyblx\n",
      "@HARUMIM_MB @KH_Vigenhoo 닮았어요..\n",
      "@pokn775 RT @mondayBlues_88: 여성기자와 컨택 했다고 합니다 찌라시 기사 클릭❌❌❌❌❌❌ #이수역_폭행사건 https://t.co/YAI5HaduCZ\n",
      "@MARKINGPAINTER RT @muttz__: #윤화평 쥬륵쥬르륵💧 https://t.co/HfYSgSkWhS\n",
      "@qn0_b8 RT @KINGOD_GUIN: 술래 황민현 (?)\n",
      "내게 무뚝뚝하게 굴지마요 견딜수가 없어요 https://t.co/n7wIlXYjGC\n",
      "@kaonattanicha RT @qqstl001: 낙엽 좋아하는 토끼 너무 힐링된다 https://t.co/OrDyMaGrEz\n",
      "@Up5Down @doridoriyamyam 이렇게 얘기하니까 급 그리워서 해보고 싶긴하네요ㅎㅎ\n",
      "@chiakitty211 RT @suga_suga__love: 옥택연 분이랑 친척오빠랑 진짜 사진 찍으심 https://t.co/ZWTeNC8dky\n",
      "@b0boIAoyJi1jXhX RT @1dhkfdhkf22: 아 군무새새끼들아!!!!!!!!! https://t.co/sHtpaCkaQe\n",
      "@mint_cyp 아 타장르 버닝중인데 현실생활이고 모고 내팽개쳐버리고 싶어서 죽겠다 크아악 덕질하는데 인생이 너무 방해댐 ㅡㅡ\n",
      "@kim_yehwan RT @DanMi_02: ㅋㅋ 우리가 이렇게 말로 한남 패고 죽이고 이러는거 다 어디서 보고 듣고 경험해서 하고있는 거라고 생각해? 허구한날 여자는 삼일에 한번 패야한다 이딴 소리 짓껄이는 놈들이 인터넷에서 불특정 다수에 얻어걸려 말로 맞앗다고 꼴값…\n",
      "@n4e74tTeGfwdDWN RT @M00M0W: 나이키 직원 불법촬영남. 집에서 120장이나 불법 촬영 사진이 발견됐는데, 회사 측에서는 피해자 분들한테 상품권 100만원 준다고 회유 시도한 사실이 밝혀짐.\n",
      "\n",
      "https://t.co/e0miS3EeHH https://t.co/…\n",
      "@jiwonmanse2017 RT @danieldotcom_: 💖KangDaniel X LAP💖\n",
      "\n",
      "LAP과 다녤이 수험생 여러분들을 응원합니다.\n",
      "마지막까지!! 으쌰으쌰 🙏🏻\n",
      "\n",
      "#랩 인스타 https://t.co/9G5xFdEyz1\n",
      "(하트꾸욱😍예쁜댓글달기❗)\n",
      "\n",
      "#LAP #LAPK…\n",
      "@yoru_joker RT @yeon_pa_rang: 서울 송파구 잠실본동 202-4 1층 모퉁이 직화구이입니다!\n",
      "\n",
      "막창 초심자도 입 짧으신 분들도 어느정도 만족하실듯😆\n",
      "저 맨날 먹는 메뉴는 생막창 1인분 12,000원.기본 동일메뉴 2인분부터 주문가능.저혼자 2인분 마…\n",
      "@Thank_U_1004 RT @haseyo_eunwoo: 아 차은우 단어선택도 자기 같이 한다고ㅠㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 집주인 맞긴한데 님이 하니까 넘 인소같고 웃기잖아요 https://t.co/BuRca1OF9H\n",
      "@IGOJKA RT @Tokki901: 아니 전정국 이거 머야ㅋㅋㄱㅋㅋㄱㅋㄱㅋㅋㄱㅋㄱㅋㅋㄱㅋㄱㅋㅋㄱㅋㅋㄱㅋㅋㅋㅋㅋㅋㅋㅋ흡사 놀이동산 https://t.co/a9LdE9lQ8G\n",
      "@ranu_hangOOVER RT @kuroxpeng: 남자가 일반화당하는게 싫고 속상하면 연대해주세요 사람패는 미친놈 이입하면서 편들어주지말고 나는저런미친놈이랑 안똑같다고 좀 보여주세요 사람의 생존여부보다 잠재적가해자취급을당해 나쁜 자기기분을 우선시하지마시구요\n",
      "@Nini3bie RT @Sugapowder_0309: 181107 푸마팬싸\n",
      "\n",
      "주목!! 애옹이 입술 봐주세요 ㅠㅠ\n",
      "우~??\n",
      "\n",
      "(수험생 여러분 수능 화이팅!)\n",
      "#슈가 #SUGA #민윤기 #윤기 #bts⁠ ⁠⁠ ⁠⁠ ⁠\n",
      " #SGPD  #방탄소년단 @BTS_twt http…\n",
      "@tthankamonn ดีมากเลยอ่ะㅠㅠ\n",
      "@cooka____ RT @O6O6_o3o: 잼동으로 동역이 결혼하는데 (구)남친 잼인이 와서 축가로 노래 한 곡 뽑고 간다. 노래는 희썽 - 결혼까지 생각했어.\n",
      "@egg_000011 RT @dksrlwns_: 수능 화이팅화이팅 https://t.co/DwiuAoX4vo\n",
      "@Iam__Fine RT @Stray_Kids: [주간아이돌]\n",
      "Stray Kids(스트레이 키즈) 랜덤플레이댄스 도전★\n",
      "\n",
      "https://t.co/4jqVLyd9sf\n",
      "\n",
      "#StrayKids #스트레이키즈\n",
      "#IamYOU\n",
      "#YouMakeStrayKidsStay\n",
      "@A_digestive_md 학교에서 수위답뎸 에바라서 뇌에힘주며 기다리고잇음\n",
      "@olivekoongya 하필 왜 지금 이 시간에... 근데 사려던 건 어차피 품절이네요....\n",
      "@SUH0_O_52 RT @my_washbowl: 제가 아기돼지 삼형제 어쩌구는 모르구요 토끼 삼형제는 알아요 https://t.co/3NKvdJKvtt\n",
      "@Ro_yu_ RT @yonhaptweet: [수능] 필적확인문구 '그대만큼 사랑스러운 사람을 본일이 없다\"\n",
      " https://t.co/WKIwbzUztR\n",
      "\n",
      "15일 시행된 2019학년도 대학수학능력시험 수험생 필적확인 문구는 김남조의 시 '편지'의 첫 구절 '그대만…\n",
      "@canladee RT @COSMIC9699: 바찌훙 미모 ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ https://t.co/X1fvgfHnvR\n",
      "@709px RT @poisonstew: 마비노기...\n",
      "하는만화... https://t.co/DiQFNvMAP1\n",
      "@angelus0501 RT @mabiapsimulator: 제일 중요한게 빠졌뀨..\n",
      "\n",
      " '환경설정 &gt; 화면 &gt; 인터페이스'에 폰트 크기 조절 옵션이 추가됩니다.\n",
      " - 채팅창과 메세지 로그, 말풍선에 출력되는 폰트 크기를 11부터 20까지 설정할 수 있습니다. https:…\n",
      "@YuU_Dil 당신 조용히해봐\n",
      "@mengmengmemeng RT @yonhaptweet: \"이런 속옷 입으면 성관계 동의라고?\"…아일랜드 여성계 '발칵'\n",
      " https://t.co/tVP1XOxtTB\n",
      "\n",
      "변론 과정에서 이 남성의 변호인이 피해자가 당시 입고 있던 속옷을 증거물로 제시하면서 한 발언이 외부로 알려…\n",
      "@prodigy_MDA RT @simtongnim: 오늘의 2컷만화\n",
      "(좌) 전여옥 왈 \"애써 기른 귤이 김정은 진상품인 걸 알았을때 농민들 심정이 어땠을까?\"\n",
      "(우) 농민들 \"우리 귤이 화해의 상징으로 북한에 간다고 할 때 너무 기뻐서 박수치고 환호했다. 긍지와 자부심이…\n",
      "@bbyo_8702 @aralau_ 빡쳐요\n",
      "@CALMyee 엘렌 섻스엔흥미없나요? &lt;점점수렁애빠지는\n",
      "@B_D0408_exo 종인이 수능 응원🐻\n",
      "너무 귀여워❤️ https://t.co/FvuAN76wS4\n",
      "@snowbird_bit RT @yey_bb: 누가 문을 쾅 치길래 봤더니 까망이가 앉아있었다.. 밥 맡겨놨냐 진짴ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ\n",
      "친구들 네마리도 데려왔엌ㅋㅋㅋㅋ큐ㅠㅠㅠㅠㅠㅠㅠㅠ귀여워서 봐줬다 https://t.co/oyVlwozoCv\n",
      "@77777771d RT @llesbiandust: 한남이 탈코한 여성에게 시비를 걸고 폭행한 사건은 이번이 처음이 아님\n",
      "유튜버 김응또님도 한남과 시비가 붙어서 쇄골이 파열됐는데 그때는 이렇게 주목받지 못했음 왜일까?\n",
      "아마 '죽을정도가 아니어서' 였던게 아닌지?\n",
      "도덕…\n",
      "@aki_fantasista RT @9256_708: 181113 #백현 #BAEKHYUN\n",
      "우리가... 냠이가... ｰ̀ㅅｰ́\n",
      "8.3mb https://t.co/8fQIMgoaBI\n",
      "4.8mb https://t.co/0xfJB8nlQs\n",
      "https://t.co/jf5dyqxfym…\n",
      "@maybe5211 같은 학교 같은 반 람찬.....급식실에서 마주보고 밥 같이 먹는 람찬.......\n",
      "@HaL_piano @Hwikaro1 오늘 개봉했어요! 꼭 보러가세요!!\n",
      "@song_sajjang ㅇㄴ 보헤미안 랩소디 봣어요 ㅇㅏ나 진자어ㅡ의ㅡ아어ㆍ의ㅡㅡ시\n",
      "@BB_XCIII RT @sweetongong: 배: 나 빨리 질문 받고 싶어\n",
      "옹: 너 해\n",
      "배: 지금 할까 그냥?\n",
      "옹: 응\n",
      "지금 이걸 먼저 하구 그 다음에 하면 될거야\n",
      "배: 으응\n",
      "\n",
      "옹: 너 하고 싶어가지구 나와가지구\n",
      "이러는 거 아니야? 그치\n",
      "배: 어 하고 싶어ㅋㅋ\n",
      "옹…\n",
      "@GLaDOSmn_krbot 시리얼(cereal). 옛날에 도급 기관에서 개발 과정에 관여한 적이 있어. Dr.J의 사설 연구동에서 피실험자들이 복도에서 이상한 짓을 하는 걸 막는 데 활용되었지. 여기선, 애퍼쳐 사이언스의 독자적인 생명유지 기술이 도입되고부터 사라졌어.\n",
      "@NCTsmtown_00 RT @_axxsh: 이런 마크 목소리 너무 사랑함 목 긁지도 꺽지도 않은 성대에서 바로 뻗어나온 목소리\n",
      "https://t.co/tNEnrs1DyS\n",
      "@__AIIO__ RT @e3w2q17946: 성냥팔이 우리엘 https://t.co/y7lJRVGw3O\n",
      "@ANGLIVEANG19 @YoshikoPantie ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ\n",
      "@Pens_ok RT @Mon_mist: 신유승 김독자 이길영\n",
      "(이름 안 달면 못 알아볼 것 같음) https://t.co/rzcSKwPZKd\n",
      "@foodie0408 RT @EXOXOXOID: [KAISTAGRAM] 181115 zdklin Instagram.\n",
      "\n",
      "박!!!!!!!!!!!!\n",
      "\n",
      "대박 나라!!!!!!!!!\n",
      "https://t.co/SGx2SSHj3h\n",
      "\n",
      "https://t.co/1JeFZ6t0GF\n",
      "https:…\n",
      "@Thank_U_1004 RT @moon_gwa: 샘들 이 마인드 잊지마세요 수능 4번 친 사촌이 쌔빠지게 강조함\n",
      "\n",
      "&gt;&gt;내가 어려우면 남도 어렵다&lt;&lt; https://t.co/SXqpm2SkCC\n",
      "@KONG0735 RT @JangandAn_3507: 티내기2 https://t.co/OyqnD9yup5\n",
      "@SEyesjr RT @cooing_dh: 강멍뭉 •ᴗ•\n",
      "#nuestw #MAMAVOTE https://t.co/tMWOlLLkua\n",
      "@dbm05089 RT @jhjh0907: #학생증제작 #진단서위조\n",
      "#통장위조 #여권제작\n",
      "#사업증위조 #졸업증명서위조\n",
      "#민증제작 #신분증위조 \n",
      "#면허증제작 #대학교졸업증위조\n",
      "#등본 #초본\n",
      "카톡상담:wwk55 https://t.co/FgbMmLcwc8\n",
      "@andalbum RT @MoppetKr: 그렇게 건강이 매우 중요한거고 운동하는 사람들을 위한 브랜드이고 뚱뚱한 사람들한테 자기관리하라고 고나리는 하면서 한국의 스포츠용품점은 왜 플러스사이즈를 팔지 않을까?\n",
      "@huranimgrimjom RT @_seekchic: \"여자들이 훨씬 감성적이라는데, 아, 그럼 세상의 모든 문인은 여자들이어야지, 왜 남자들이 글을 쓴당가\"ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ…\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWantReadError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\OpenSSL\\SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1813\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1814\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\OpenSSL\\SSL.py\u001b[0m in \u001b[0;36m_raise_ssl_error\u001b[1;34m(self, ssl, result)\u001b[0m\n\u001b[0;32m   1613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_ERROR_WANT_READ\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1614\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mWantReadError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1615\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0merror\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_ERROR_WANT_WRITE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWantReadError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-a2e4f1d5d528>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# 공개돼 있는 트윗을 샘플링한 스트림을 받습니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# 키워드 매개변수인 languages로 한국어 트윗만 추출합니다\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ko'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;31m# stream.sample(languages=['en'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, async, languages, stall_warnings)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstall_warnings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stall_warnings'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'true'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masync\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m     def filter(self, follow=None, track=None, async=False, locations=None,\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_start\u001b[1;34m(self, async)\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    264\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnooze_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnooze_time_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_connect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mssl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                 \u001b[1;31m# This is still necessary, as a SSLError can actually be\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_read_loop\u001b[1;34m(self, resp)\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                 \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# keep-alive new lines are expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36mread_line\u001b[1;34m(self, sep)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_chunk_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    382\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m                     \u001b[1;31m# Close the connection when no data is returned\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_readinto_chunked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_readinto_chunked\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m                 \u001b[0mchunk_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_chunk_left\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mchunk_left\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtotal_bytes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_get_chunk_left\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    544\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# toss the CRLF at the end of the chunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 546\u001b[1;33m                 \u001b[0mchunk_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_next_chunk_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    547\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_next_chunk_size\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_next_chunk_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[1;31m# Read the next chunk size from the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"chunk size\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWantReadError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m             \u001b[0mrd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The read operation timed out'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\urllib3\\util\\wait.py\u001b[0m in \u001b[0;36mwait_for_read\u001b[1;34m(socks, timeout)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mor\u001b[0m \u001b[0moptionally\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0msocket\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpassed\u001b[0m \u001b[1;32min\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     sockets that can be read from immediately. \"\"\"\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wait_for_io_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEVENT_READ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\urllib3\\util\\wait.py\u001b[0m in \u001b[0;36m_wait_for_io_events\u001b[1;34m(socks, events, timeout)\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         return [key[0].fileobj for key in\n\u001b[1;32m---> 26\u001b[1;33m                 selector.select(timeout) if key[1] & events]\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\urllib3\\util\\selectors.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m             \u001b[0mready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             r, w, _ = _syscall_wrapper(self._select, True, self._readers,\n\u001b[1;32m--> 320\u001b[1;33m                                        self._writers, timeout)\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\urllib3\\util\\selectors.py\u001b[0m in \u001b[0;36m_syscall_wrapper\u001b[1;34m(func, _, *args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m         and recalculate their timeouts. \"\"\"\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0merrcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\urllib3\\util\\selectors.py\u001b[0m in \u001b[0;36m_select\u001b[1;34m(self, r, w, timeout)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[1;34m\"\"\" Wrapper for select.select because timeout is a positional arg \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mselect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tweepy\n",
    "\n",
    "# 환경변수에서 인증 정보를 추출합니다.\n",
    "# CONSUMER_KEY = os.environ['CONSUMER_KEY']\n",
    "# CONSUMER_SECRET = os.environ['CONSUMER_SECRET']\n",
    "# ACCESS_TOKEN = os.environ['ACCESS_TOKEN']\n",
    "# ACCESS_TOKEN_SECRET = os.environ['ACCESS_TOKEN_SECRET']\n",
    "CONSUMER_KEY        = \"Rh9PPrGkDRohh5GualkQRErPu\"\n",
    "CONSUMER_SECRET     = \"Emg0dTXJXawpWn4EapiyG2aE5bWHGLreFd6vPy4XZMQmVb7ULp\"\n",
    "ACCESS_TOKEN        = \"4477564098-AGJFdwERnNyEqVGHaPyy8yyloiPaCXsPob1q1Zy\"\n",
    "ACCESS_TOKEN_SECRET = \"otjvkl1U6POGZMw1SpGQHMHlyEPz9Z1TbhPIecFhCuI6e\"\n",
    "\n",
    "# 인증 정보를 설정합니다.\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    \"\"\"\n",
    "    Streaming API로 추출한 트윗을 처리하는 클래스입니다.\n",
    "    \"\"\"\n",
    "    def on_status(self, status):\n",
    "        \"\"\"\n",
    "        트윗을 받을 때 호출되는 메서드\n",
    "        매개변수로 트윗을 나타내는 Status 객체가 전달됩니다.\n",
    "        \"\"\"\n",
    "        print('@' + status.author.screen_name, status.text)\n",
    "\n",
    "# 인증 정보와 StreamListener를 지정해서 Stream 객체를 추출합니다.\n",
    "stream = tweepy.Stream(auth, MyStreamListener())\n",
    "\n",
    "# 공개돼 있는 트윗을 샘플링한 스트림을 받습니다.\n",
    "# 키워드 매개변수인 languages로 한국어 트윗만 추출합니다\n",
    "stream.sample(languages=['ko'])\n",
    "# stream.sample(languages=['en'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c5-05_streaming_api_with_tweepy \n",
    "- Tweepy 로 Streaming API 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@MD_pingnew @mxplaxit 따지자면 버그인게 맞으니 다시 고쳐주진 않겠죠..ㅠㅜ 스샷노기 안녕..\n",
      "@dreamingu_mnks @72_Umezz 이 힝구에겐 당나귀귀 하셔도 무방합니다\n",
      "@k_h_0602 🐻스타그램 \n",
      "\n",
      "🐻「대」\n",
      "\n",
      "#EXO #KAI #종인 #카이 https://t.co/05TtrIoRAX\n",
      "@yutaisacutecat RT @jobdukeuni: 산이 미쳤나 ?? https://t.co/NXY6hqECZX\n",
      "@KSH_JNF @Cake_JNF 아햐햐햨! 그래?? 화학화학!하고 웃는것보다는 이게 낫지않아? 특이한 웃음소리 듣고 구조하러 오면 좋겠다!!!그럼! 화학을 좋아하지 않으면 입주 불가능한 마을이라고! 완벽한 마을이지! ...?아니 조난은 재미있진 않지.\n",
      "@1495beer 토요낮공 씅풍이잖아\n",
      "@myforestO RT @theMEGASTUDY: 어서와 이렇게 영롱한 플래너는 처음이지? ٩( ᐛ ) و \n",
      "\n",
      "#비교불가 #반박불가 역대급 플래너가 왔댜 🎉\n",
      "\n",
      "속지도 업그레이드! 메가쌤+대학로고 스티커까지!\n",
      "안 갖고는 못 배기는 소. 장. 필. 템!🔥✨🤟🏻\n",
      "갖고 싶지…\n",
      "@_madestiny RT @0u0pp: Wanna One “1¹¹=1(POWER OF DESTINY)” \n",
      "\n",
      "1.Destiny (Intro.)\n",
      "2.봄바람 *TITLE\n",
      "3.집\n",
      "4.불꽃놀이 (Flowerbomb)\n",
      "5.묻고싶다 (One Love)\n",
      "6.Deeper\n",
      "7.술래 ht…\n",
      "@HikaRi_lights @sulhwa_0808 ㄱㅋㅋㅋㄱㅋㅋㅋㅋㅋㄱㅋ 이게 말로만 듣던 재능낭비.?ㅋㅋㄱㅋ\n",
      "@Eli_natsu @mybaekgimylife 진짜죠?? (울먹\n",
      "@xoxohugnkiss RT @warm_garcon: ㅜㅜㅜ💛💛 https://t.co/BzrbFRTSfI\n",
      "@Heen_foll 그런데 말입니다.왜 나랑 친해지려 하지 않을까요. 그것이궁금하다.\n",
      "@1im_sujung 서울 인천 사시는 분들 중에 팝콘 콜라 꽁짜로 드셔주실분...?? 엄마한테 선물 주고 싶따요...ㅎ https://t.co/VZQS6mQMv9\n",
      "@ONG_95825 RT @_stardust365: 181115 아미고TV 개인영상\n",
      "https://t.co/fqHTMXCaa0\n",
      "#옹성우 #ongseongwu https://t.co/LYVkgbM0j9\n",
      "@sehun_0326 헉 씽스타\n",
      "@hWfsi85eTKcTz4U 아 혈압올라 거래처담당자 싫어\n",
      "@pCqo4i0Y7kD0aTN RT @mapocheetah: 멜로디....뭐야 지금 좀 감동이에요.... 한다면 이렇게나 잘 할 수 있잖아...? 하지만 긴장을 놓으면 안 돼요 우린 꼭 확실한 1등을 선물해줘야 해요ㅠㅠㅠ 오늘자 투표 꼭꼭 다 때려박고 자정 지났으니 스밍 다시 확…\n",
      "@Dasa_LnP RT @eunwi_lnp: 허묵 생일 합작 참가했습니다! 교수님 아프지말고 행복하자!\n",
      "아래는 멋지고 아름다운 연성이 가득한 곳!!\n",
      "▷▷https://t.co/4fptDWwFY8◁◁\n",
      "#교수님생일축하해요 #허묵 https://t.co/rqWqWd1IFs\n",
      "@kai_mar_zkdl 뭐야아....뭐야아ㅠㅠㅜㅠㅠㅜ https://t.co/yubbajEb19\n",
      "@dae_0129_hwi @Sol2_991102 으 으 내 작년담임 개싫어 으 으\n",
      "@94loverr 이 사진 너무 조아 ㅠ 이고바 물꼬기바 ! 이고냐구 ㅠ 진짜 세젤따랑둥아 ㅠ💕💕💕 https://t.co/GsNkvcAAPX\n",
      "@nnuisirikwan RT @mylittlesono: #유선호 \n",
      "존잘 청소년이 본인의 잘생김을 수용하는 과정에 대한 랜선 관찰 https://t.co/STTzZuwj3B\n",
      "@exo050408 RT @whddlss20114: 샘들 mma 아까까지만 해도 1위역는데 몇시간만에 밀렸어 이거 어쩔거야,,,,,,          투  표해    투표      일초도안걸ㅇ려 https://t.co/a8odzo141M\n",
      "@freshpeach0129 @ONGOng_8250 ㅠㅠㅠㅠㅠㅠ\n",
      "@werther_0 그럼 내한하라고\n",
      "ㅠㅠㅠㅠㅠㅠ https://t.co/I6LddbnoFA\n",
      "@kirishimaikuya3 @ssk_duck ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 선굿즈 후애니 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 대단하신 숙해님\n",
      "@2EUNBIYA RT @zoodi0419: 인스타에 언급하니 아는 오빠가 '그니까 넌 죽겠다ㅋㅋㅋㅋ'라고 댓글을 달았다 이게 정상인가?\n",
      "#이수역_폭행사건 https://t.co/L6gItHXY50\n",
      "@HOPPANG_00 @chltmd9982 시험도 안끝나다니 부짱해라... 안녕....\n",
      "@Esteban_348 RT @Noljadang: 전국의 00년생 여러분\n",
      "수능 끝낫다고 고삐 풀리지 마세요.\n",
      "\n",
      "1월까지만 기다리면 술 합법적으로 살 수 있으니까 제발 기다려주세요.\n",
      "\n",
      "여러분들의 그 욕심에 한 가정의 생계가 무너질수도 있습니다.\n",
      "\n",
      "실제로 저희 어머니 지인분이…\n",
      "@Krk75xao7todMXt RT @hardy_luv: 불어하는 톰하디 보세요. 영하디 진짜 아름다운데 불어까지함 웃는것까지 완벽하다 https://t.co/almyvhXK5C\n",
      "@fkaus33 RT @songnun1004: 손썩을뻔한 만화 백업 https://t.co/kKC2G3YQUw\n",
      "@Dr_Moo__moo RT @govideotravel: 출발 비디오 여행에서 '불법 촬영 기기 판매 중지 및 규제 강화'에 대한 청원을 올렸습니다. 많은 rt와 참여 부탁드립니다. rt추첨 한 분께 베스킨라벤스 기프티콘을 드립니다.\n",
      "https://t.co/UUy6rnKF…\n",
      "@jeam___ RT @Action_Sang: 이번 수요일과 목요일은 3가지로 분류가 가능하다\n",
      "\n",
      "초딩 : 와 ㅅㅂ 부럽다ㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏ\n",
      "\n",
      "중고딩 : 아 개꿀 너무좋다ㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏ\n",
      "\n",
      "고3 : 씨발롬들아ㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏ\n",
      "@Raminie4869 RT @MoncherDo: 물을 왜이렇게 귀엽게 마시는즤,, (ᗒᗣᗕ)՞ \n",
      "\n",
      "#경수 #디오 #스윙키즈 #kyungsoo https://t.co/UnL9oTkLAl\n",
      "@fakeluv_v RT @Adonis_Hoseok: 181106 MGA\n",
      "이분위기 무엇ㅠㅠㅠ 와,,,😭😭😭순이 왜케 이뻐ㅠㅠㅠㅠㅠㅠㅠㅠㅠ\n",
      "#제이홉 #정호석 #JHope\n",
      "#방탄소년단 #BTS⁠ ⁠⁠ ⁠⁠ ⁠⁠ ⁠⁠ ⁠⁠ ⁠⁠ ⁠⁠ ⁠⁠ ⁠⁠ ⁠⁠ ⁠⁠ ⁠\n",
      "@BTS_twt…\n",
      "@S205_4 RT @muka_odinson: 작년에 써서 보냈는데 오늘 또 가족관계단절사유서 보내라고 우편 왔다. 웃긴 건 안에 가족관계단절사유서 양식도 안 들어 있어... 내일 또 구청에 전화해야 하는데 또 머리가 지끈지끈 아프고 눈물이 쏟아지려고 해...\n",
      "@soseulkim RT @band27_vo: 취해서 몸집 작은 아주머니 끌고가 때려 죽이고, 헤어지자고 하니 죽이고, 초등 여아 강간하고 합의하에 가진 성관계라 말하고 술집 옆테이블에서 기분 나쁘게 한다고 피나도록 여자를 때리는 지금 이 상황이 사실이고 현실인걸 이제…\n",
      "@YangsongE_ RT @mong9_Kim: 토끼모자 최강자 ㅋㅋㅋㅋㅋㅋㅋㅋ https://t.co/DxuNH77Sgu\n",
      "@nctIisa RT @myfirstlovebp08: 진짜 앨범 두개밖에 안나왔지만 역대급이야 https://t.co/hKnJTosPq5\n",
      "@fabledxo 종인말투 뭐야아 https://t.co/bWhhKYr7jk\n",
      "@ljyeaster2 최근 꽤 열심히한 게임\n",
      "\n",
      "각부분의 파츠를 조립해서 자기만의 머신을 만들어 상대방과 겨루는 게임. \n",
      "\n",
      "초반엔 진짜 파츠만 잘 조립하면 판도를 뒤집을수도 있고 상성도  짜임새있어서 이리저리 조립하는 재미가 있었… https://t.co/IlebUZ4liC\n",
      "@KSH_JNF @Cake_JNF 너는 언제 조난당할지 몰라서 재미있어??? 평소에 나는 특이하다는 소리 많이 듣는데 내 생각에는 나보다 네가 더 특이한것같아!\n",
      "@FF14_Twinkle RT @ventuno73: 이 냥이의 근황 https://t.co/06FMhKYhtx\n",
      "@redcrumble326 [#멜론뮤직어워드] “서리서리...”님이 투표한 #댄스(남) 부문 후보 #花요일(Bloomi..., 현재 1위로 2위와 112,039표 차이 입니다.(18.11.15 13:44)… https://t.co/EvwkwnMhMe\n",
      "@reaganSS22 RT @carpe794: 던지는 말마다 다 맞음ㅋㅋㅋㅋ https://t.co/Cj5FyKEbHD\n",
      "@chazzima 오우 https://t.co/vn36aP08nB\n",
      "@ihavecloud35 RT @ddowoopp: 아니 메이킹드라마 티켓 뭔데... 진짜 이런 말 하게 싫었는데 너네 일안해...?ㅠㅠ https://t.co/xM6xNHYRzx\n",
      "@koro7324 RT @strange_sher: 피터가 생각하는              다른 사람들이\n",
      "자기가 화내는                  보는 모습\n",
      "모습 https://t.co/BoeNe7QHX6\n",
      "@riskydiver19 RT @20thCFoxKR: “한국 관객 분들의 사랑에 정말 행복합니다”\n",
      "#역대급 #역주행 흥행 감사 인사 영상 최초공개▶\n",
      "2018년 최고의 역주행 흥행작!\n",
      "\n",
      "#보헤미안랩소디 #절찬상영중 #역대급역주행\n",
      "#최고의찬사 #배우내한기원_N차관람각 https…\n",
      "@Jiwon31885612 @TcgGZ2RxKN4a0ET 뎀 봐주세요!\n",
      "@Loveyisoonsin @TexasRanger01 아직 그 부분은 뭐 나온게 없으니 피카츄 배랑 노는게 현재는 답이라는거.\n",
      "@glimja119 정말.... 정말 하기 싫겟다 천년의 욕정이 식을듯 (\n",
      "@_104652 RT @SeaLeydoux_: 어떻게 제목을 이따위로 뽑을 수 있지? 언론이 나서서 피해자가 맞을 짓 했네 프레임을 씌우고 있음 https://t.co/9LwxFESIl6\n",
      "@y0lolife 너란 트랙 위 \n",
      "나를 얹을게\n",
      "\n",
      "어떤멜ㄹ로딜지\n",
      "상상을해    봐  ~~~ ~~~~~~~~~~~~\n",
      "@Meli_Oz9999 RT @wns428: 99.9923%의 빛을 흡수하는 가장 어두운 검정색 (맨 오른쪽)\n",
      "\n",
      "손가락이 안보일 정도로 강한 빛을 비춰도 마치 구멍 뚫린 듯 새까만 모습을 유지하고 있다\n",
      "\n",
      "https://t.co/w5PAThGrfc https://t.co/D…\n",
      "@Fifth_Person_ 모나크, 집탄률, 과학\n",
      "\n",
      "#WOWS https://t.co/1eDRxy4uP7\n",
      "@le8b0by4b 임헌일, 오는 일 발매 '년 만에 솔\n",
      "👐 🌜 💘\n",
      "9qonwb-8w0t\n",
      "@letter_ing 누가 눈치없이 박 썼어 누구야\n",
      "@moon_101806 RT @srq4mgplDxK74ZO: #디크런치_세븐틴_표절_피드백_요구\n",
      "지금우리 캐럿들하고 싸우자는거에요 아니면 뭐하자는거에요 피드백 그거하나 올려주기 힘들어요? 이걸보고 세븐틴이 어이없어하면 어쩔건데요. \n",
      "무논리 피드백올리시면 진심으로 저작권침해…\n",
      "@Fxxk_pepsi 피구공 커-엽\n",
      "@namcher030 RT @kimjjunim: (간략히)아칼리를 작업하신 아티스트께서 9개월의 작업기간동안 스트레스로 패닉어택을 격고 병원까지 다니셨으며 그사이\n",
      "매니저에게 구두로 승진과 연봉인상등으로 말뿐인 회유를 당하고 정작 끝내는 아무것도 없었으며 신캐발표파티에도…\n",
      "@Amazing95V [#멜론뮤직어워드] “amaz...”님이 투표한 #베스트송상 부문 후보 #FAKELOVE, 현재 1위로 2위와 155,615표 차이 입니다.(18.11.15 13:45) https://t.co/JF6BlFx90A #Melon\n",
      "@Lxxxris 기분이 게이 같아서 조퇴하고 싶어요\n",
      "@_v_ivi__ RT @SMmakesitLive: 🌟!t Live(잇라이브) : 시.아.코\n",
      "EXO와 함께하는 MUGI-BOX🤔\n",
      "🔷 시청자 아이디어 코너 🔷\n",
      "\n",
      "엑소에게 궁금한 질문이 있다면\n",
      "간단하게 댓글로 달아주세요!\n",
      "\n",
      "오는 11월 18일 오후 6시에 진행되는\n",
      "잇라이…\n",
      "@GraziaM72225843 RT @nowornever0616: 곧 다가올 마법같은 시간 미리보기 #남자친구 #박보검 #김진혁 https://t.co/aZY2Sj6YrF\n",
      "@wirt_l 쨘 ★ https://t.co/U3QYWcKS4d\n",
      "@cherish__950808 RT @pledis_17: [17'S 도겸] 너무 재밌었던 세븐틴의 TTT\n",
      "출바알~~!!&gt;_&lt; https://t.co/1ws11tnsQw\n",
      "@iNAM12445 RT @springginsour: 뽀둥뽀둥 뽀쟉뽀쟉..💕\n",
      "\n",
      "#StrayKids #스트레이키즈 #현진 #황현진 #Hyunjin https://t.co/EtpoFN4KjT\n",
      "@88take0114 김죠닌,, 귀여움 너무 과해,, 반칙이야,,\n",
      "@cody_9_ 이제... 얼마남지 않앗다고...하오!!!!!!! 날로 꺼지는 몸 악몽 같은ㄴㅏ날~ 이 괴로움을 잊게 해주는 당신을 잃을까 두려워서 차마 말할 수 없었어~\n",
      "@Eti_nct RT @Mooonopal: 혐오범죄의 효과는 여실하다. 맞아도 싼 여자와 그렇지 않은 여자를 구분하고 여자들이 스스로를 검열하게 하여 '맞아도 싼' 여자처럼 보이지 않게 노력하도록 만드는 것. 이 세상에 맞아도 싼 여자는 없다. 오로지 남자의 비위를…\n",
      "@sosweet_j 복5십따😭 촤령 하는 중에도 새벽에 공ㅋr와서 글남기고 고먀ㅁ1 사진 올려주고 덧글 달아주고😭 이런 아ㅇ1돌이 어디있음?😭 https://t.co/fe3Y3BpwI4\n",
      "@_gentle_breeze 뭐야아 라니 너무 귀엽다아 https://t.co/xVqFbcR9Rr\n",
      "@nu_carrot 기여어아ㅏ어어어\n",
      "@saranjiwoo RT @nightswancastle: 정부 비판을 문재인대통령비판으로 받아들이지 말라고 하시니\n",
      "제가 하는 이해찬 ㅂㅣ판도 민주당 망해라고 받아들이지 마셨으면...\n",
      "@Reumha_for_u @anhwamuk_ 저거 마치라잌 현생에 찌든 김름같아요 아마도 내일의 골골킹 김름. .. .. ......\n",
      "@wannable_yeji RT @winkstar99: 아미고(영상=행바 님) #박지훈 \n",
      "#MAMAVOTE #wannaone \n",
      "전하가 웃으십니다😆😆😆\n",
      "\n",
      "O https://t.co/gfye0OPKEH\n",
      "X https://t.co/WnIFuBFPLY https://t.co/9CjD…\n",
      "@pocachip_green RT @playweedme: 가수 이요한씨 인스타 스토리 무슨 뜻인가요 https://t.co/ot3ZhhB6V6\n",
      "@Radicalghktxl RT @hankookilbo: 실업률 IMF 이후 최악...40·50대 거리 내몰리고 자영업자도 감소\n",
      "\n",
      "실업자 수는 97만3,000명\n",
      "\n",
      "https://t.co/T8nT7WLduG https://t.co/HH6aK9WNZU\n",
      "@charmwon @Anes_HJ 후훗 후훗 아네스님 빠부빠부~~~~~!!!\n",
      "@Hae_ff14 속쓰려 짬뽕 먹어야지\n",
      "@nn_Iseek_A 그냥 첫시간끝나고 교수님께 말하고 나와야지..ㅜ 진심 급하다 졸작..\n",
      "@ArashiForD_ream RT @shira_jeong: 여자라고 안봐준다 난 여자도 팬다하는 놈들 특징은요 사실 남자는 못 팸\n",
      "@ldyaaaa RT @meg4015: 으하하하 오늘 받았당 귀여운양말과 몬베베패치 너무너무 감사합니당❤️❤️❤️👍😍@hQi1c5TOV4mQXDA https://t.co/s70HqHPLnA\n",
      "@HACLOUD_9422 RT @io_oi_121: 아니 샹 미친거아니냐고 https://t.co/BiHFoqpd9l\n",
      "@lavenevv RT @juniecho: 아니 존나 상상해보면 개 웃기지 않나요 그렇게 사람을 집단으로 패고다닐 정도 퀄리티의 놈들이 평소에 퀴어혐오를 과연 안했을 것 같습니까? 근데 그런 놈들이 '너 게이냐' 라고 물어오니 분노해서 사람을 팼다면 그 분노는 어디서…\n",
      "@tasty_tangerine 비ㅋㅋㅋㅋㅋㅋㅋ페ㅋㅋㅋㅋㅋ밐ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ걍 성차별주의자라 하세얘~~~~~~\n",
      "@omd1120 RT @pearlblow: 헌법에 기록된 종교의 자유를 무시하는 한기총 소속 목회자들\n",
      "\n",
      "▶️ https://t.co/MFmumZaA2y\n",
      "\n",
      "#신사참배 #부패한_한국기독교 \n",
      "\n",
      "https://t.co/zeKhjp1kDX\n",
      "@pockyjrbugi RT @bomangx: 애기강아지같애ㅜ복슬복슬 ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ기여워ㅠㅠㅠㅠㅠㅠㅠ https://t.co/tuGwK1eQCu\n",
      "@C_rown_sando RT @sashimi0420: 짤트레👀.. 원본짤 보자마자 아이자와 생각남ㅋㅋㅋㅋㅋ\n",
      "\n",
      "- 가라, 제군들. https://t.co/Kmw05NVM1C\n",
      "@lindalinda0321 RT @artpurpleJ: 스케줄 꽉차서 갈 확률도 거의 없던 홍백소식 쏟아내는 언론들 지난 연말 소년단이 지상 최대 연말쇼 딕 클락스 뉴 이어즈 락킹이브 출연해서 핫타임에 방송 나간 건 대대적으로 보도하셨었나용🤔 무려 이 것도 스케줄이 바빠 사녹…\n",
      "@wmw5447 RT @Exa_F1are: 1. 때리지마\n",
      "2. 강간하지마\n",
      "3. 죽이지마\n",
      "4. 욕하지마\n",
      "\n",
      "이거 네개를 못한다고?????? 진짜????????????? 진짜????????????????????????? 그러고도 너네가 지성체냐?????? 왜살어 걍 죽지…\n",
      "@HP____808 RT @KEH_123123: 난 이 둘이 같은 나이라는 것을 알아버렸다 https://t.co/VRf6khKI58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@asoka0 RT @BH_Hong2: 헉 진짜...칸예웨스트가 백현이 팔로했어...!!!🙊 https://t.co/FrwdW7Xkbw\n",
      "@ttk0625 RT @anna57148315: 우리 입더러운 정마으문 신고 한번씩 해주세요 ~ https://t.co/UMmSoEa5mY\n",
      "@Lin_a_bsd 아 히구치 귀여워ㅠㅠㅠㅠㅠ나말고도 히구치가 더 있던가 몬데ㅠㅠㅠㅠㅠㅠㅠㅠ선배를 좋아하면서도 선배가 자기를 좋아할거라 생각하지 못하는 우리 히구찌\n",
      "@SriEndang_yani RT @StoneMusicEnt: 처음으로 보이는 워너원 멤버가 내 운명♥\n",
      "@야, 내 운명 옹성우래~ 넌 누구나왔음??? \n",
      "　\n",
      "[운명의 단어 찾기] Wanna One (워너원)\n",
      "　\n",
      "#WannaOne #워너원 #20181119_6PM\n",
      "#POWEROFD…\n",
      "@Mint_suga_o 포토티켓 어떻게 뽑아여??\n",
      "@MeatMAC2 @WingSett 앗 완료 캡처 안 했는데ㅜ 암튼 했음~~ 팀플 힘내🤗💥💥\n",
      "@penguiny22 RT @masigy: 같은 사람으로 안 보니까 그럴 수 있는거야.\n",
      "사람이 아니라 당신보다 약한 '물건'이라고 생각하니까.\n",
      "자기가 자기 인생 말아먹고나서야 후회한다? 그것도 똑똑한 사람들이야 알지 너넨 남탓하잖아.\n",
      "심지어 피해자분들이 먼저 시비를 건…\n",
      "@Blooming_Day_92 본격 수험생 없는 수능응원방송\n",
      "@ayangmi1111 RT @Pianjue_: 아직도 자기가 새끼 때 몸인 줄 아는 대형견같아 너무귀여워ㅜㅠㅠㅠㅠㅠ https://t.co/3IpO44dKaX\n",
      "@na_yoon_twt @CGVADGT1230 훟핳훟핳 기대가 됩미다\n",
      "@jpsoo1565 RT @bhforever56: 칸예웨스트가 백현이 팔로했어 🙊🙊 https://t.co/yvegfmj31s\n",
      "@dbwlsdlek_ RT @fangsongxo: 엑소, 美빌보드 메인차트 '아티스트100' 9위+TOP10 첫 진입[공식입장] | 다음연예 https://t.co/6avHgQUP7L\n",
      "@thfthf6972 RT @712_ATM: 케모노사키 https://t.co/5V1Mt19pms\n",
      "@CHOIGAEMO RT @thirdstaaar: 두영이랑 드림애들 한강놀러갓을때..나재민은 어땠냐고 말썽 절대안피울상이긴ㄴ한데 조용했는지 아니면 동군이랑 같이 오지게놀아재낀건지\n",
      "@wjdqhr_fl027 RT @MOSS_DH_129: Awake! #이대휘\n",
      "Everyday 모든 게 따분했었던 나의 하루에 스며들었죠 내가 더 설레게 https://t.co/SnTjrg9XNM\n",
      "@kakchan61 RT @Sabor_a_DO: 181112 그대의 모습에는 서사가 있다. \n",
      "\n",
      "#경수 #디오 #도경수 #KYUNGSOO #DO #DohKyungsoo D.O.\n",
      "#스윙키즈\n",
      "@weareoneEXO https://t.co/qSyvVtECB9\n",
      "@LeanFat RT @otonacool: 지금 조금 이상한 플로우가, #이수역_폭행사건 의 피해자가 먼저 시비 털었다며 그 내용이 스샷과 돌면서 '문제'되는거. 왜 여자들이 나서서 '맞아도싸다'에 힘을 실어주고 있는지 모르겠네. 워마드를 싫어한다고 해서 그들이 당…\n",
      "@polarxnacht 없어... 야옹이... 냥...\n",
      "@vesania_loba RT @fyhotshot: [🌸] ‘니가 미워’ debuted on #85 on Bugs Chart! #핫샷 https://t.co/NdbOgx9qIv\n",
      "@Reincarnated_P @hoong0515 라고 명문대생이 말했습니다.\n",
      "@loveyou_navil 하....시험 100일남음\n",
      "@kjh_050227_ RT @1013_BTS__JIMIN: 푸리댄스 아주 미쳐서 죽죠 죽어 애기에서 왕방멋있는 지민이 된다니까요 https://t.co/wUY5STE3pT\n",
      "@kuroka_restart 아직 사용하지않은 거라면 코우 만들까........\n",
      "코유키는 이미있어........(원래 내가 가지고있는 메모리패스)\n",
      "@170201_0308 RT @xxddm0326: 드디어 기다리던 펜들이 다 왔고 잉크도 맘에 드는 걸로 싹 넣었다!!! 특히 마지막으로 들인 #IWI 가 생각보다 너무 좋아서 조만간 핸드스크립 또 살지도.....사진만 봐도 배가 부르다 음하하\n",
      "#내펜에잉크뭐넣지 #블루블랙…\n",
      "@vi_dabi @UA_Bakugo_No1 아직 한명도 안겹쳤어.\n",
      "@purewhittte_ RT @sae7809: 아나 ㅠㅠ 아나ㅠㅠㅠ 가만안둬 가만안둘래 최윤 입속에 넣고 와랄랄라해버릴래 https://t.co/IGqygtQ1i9\n",
      "@seo_wuon RT @flowercracker: ㅋㅋㅋㅋㅋㅋ'언어강간'이라는 표현을 기사에서 보다니..ㅋㅋㅋㅋ 여자가 들을 땐 '네가 참고 넘어가' 부류에 속했던 말들이 저렇게 무섭고 자극적이고 폭력적인 기사 제목으로 쓰일 줄은 몰랐네...허 참ㅋㅋㅋㅋ(그냥 말이…\n",
      "@kimddoddi_bot 어.. 쟈르 보이므? 궁금한게 있는데.. 지금 제 손 보이죠? 이거 몇개?\n",
      "@wsmwsmasyasy RT @_dada105: 여자들은 그놈의 ‘페미 무서워서 뭔 말을 못하겠다’는 남자들에게 폭행 당할까봐, 죽임 당할까봐 늘 불안에 떨어야한다. 이런 사회에서 대체 역차별이란 말은 어떻게 나오고 여성 상위시대라는 말이 어떻게 나올 수가 있냐 #이수역_…\n",
      "@markswhore RT @got7_0116_0106: 나고야 맠라이언보이🦁 https://t.co/tba4l8QeXx\n",
      "@DonaldDuckSays 만날 물먹는것도 지루하다 지루해 꽉!ㅡㅡ\n",
      "@D_third92 씽스타ㅜㅜㅜㅜㅜ\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWantReadError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\OpenSSL\\SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1813\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1814\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\OpenSSL\\SSL.py\u001b[0m in \u001b[0;36m_raise_ssl_error\u001b[1;34m(self, ssl, result)\u001b[0m\n\u001b[0;32m   1613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_ERROR_WANT_READ\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1614\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mWantReadError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1615\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0merror\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_ERROR_WANT_WRITE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWantReadError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-a2e4f1d5d528>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# 공개돼 있는 트윗을 샘플링한 스트림을 받습니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# 키워드 매개변수인 languages로 한국어 트윗만 추출합니다\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ko'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;31m# stream.sample(languages=['en'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, async, languages, stall_warnings)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstall_warnings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stall_warnings'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'true'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masync\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m     def filter(self, follow=None, track=None, async=False, locations=None,\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_start\u001b[1;34m(self, async)\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    264\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnooze_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnooze_time_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_connect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mssl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                 \u001b[1;31m# This is still necessary, as a SSLError can actually be\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_read_loop\u001b[1;34m(self, resp)\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                 \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# keep-alive new lines are expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36mread_line\u001b[1;34m(self, sep)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_chunk_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    382\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m                     \u001b[1;31m# Close the connection when no data is returned\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_readinto_chunked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_readinto_chunked\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m                 \u001b[0mchunk_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_chunk_left\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mchunk_left\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtotal_bytes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_get_chunk_left\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    544\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# toss the CRLF at the end of the chunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 546\u001b[1;33m                 \u001b[0mchunk_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_next_chunk_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    547\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_next_chunk_size\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_next_chunk_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[1;31m# Read the next chunk size from the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"chunk size\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWantReadError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m             \u001b[0mrd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The read operation timed out'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\urllib3\\util\\wait.py\u001b[0m in \u001b[0;36mwait_for_read\u001b[1;34m(socks, timeout)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mor\u001b[0m \u001b[0moptionally\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0msocket\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpassed\u001b[0m \u001b[1;32min\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     sockets that can be read from immediately. \"\"\"\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wait_for_io_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEVENT_READ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\urllib3\\util\\wait.py\u001b[0m in \u001b[0;36m_wait_for_io_events\u001b[1;34m(socks, events, timeout)\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         return [key[0].fileobj for key in\n\u001b[1;32m---> 26\u001b[1;33m                 selector.select(timeout) if key[1] & events]\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\urllib3\\util\\selectors.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m             \u001b[0mready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             r, w, _ = _syscall_wrapper(self._select, True, self._readers,\n\u001b[1;32m--> 320\u001b[1;33m                                        self._writers, timeout)\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\urllib3\\util\\selectors.py\u001b[0m in \u001b[0;36m_syscall_wrapper\u001b[1;34m(func, _, *args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m         and recalculate their timeouts. \"\"\"\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0merrcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3-52\\lib\\site-packages\\urllib3\\util\\selectors.py\u001b[0m in \u001b[0;36m_select\u001b[1;34m(self, r, w, timeout)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[1;34m\"\"\" Wrapper for select.select because timeout is a positional arg \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mselect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tweepy\n",
    "\n",
    "# 환경변수에서 인증 정보를 추출합니다.\n",
    "# CONSUMER_KEY = os.environ['CONSUMER_KEY']\n",
    "# CONSUMER_SECRET = os.environ['CONSUMER_SECRET']\n",
    "# ACCESS_TOKEN = os.environ['ACCESS_TOKEN']\n",
    "# ACCESS_TOKEN_SECRET = os.environ['ACCESS_TOKEN_SECRET']\n",
    "CONSUMER_KEY        = \"Rh9PPrGkDRohh5GualkQRErPu\"\n",
    "CONSUMER_SECRET     = \"Emg0dTXJXawpWn4EapiyG2aE5bWHGLreFd6vPy4XZMQmVb7ULp\"\n",
    "ACCESS_TOKEN        = \"4477564098-AGJFdwERnNyEqVGHaPyy8yyloiPaCXsPob1q1Zy\"\n",
    "ACCESS_TOKEN_SECRET = \"otjvkl1U6POGZMw1SpGQHMHlyEPz9Z1TbhPIecFhCuI6e\"\n",
    "\n",
    "# 인증 정보를 설정합니다.\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    \"\"\"\n",
    "    Streaming API로 추출한 트윗을 처리하는 클래스입니다.\n",
    "    \"\"\"\n",
    "    def on_status(self, status):\n",
    "        \"\"\"\n",
    "        트윗을 받을 때 호출되는 메서드\n",
    "        매개변수로 트윗을 나타내는 Status 객체가 전달됩니다.\n",
    "        \"\"\"\n",
    "        print('@' + status.author.screen_name, status.text)\n",
    "\n",
    "# 인증 정보와 StreamListener를 지정해서 Stream 객체를 추출합니다.\n",
    "stream = tweepy.Stream(auth, MyStreamListener())\n",
    "\n",
    "# 공개돼 있는 트윗을 샘플링한 스트림을 받습니다.\n",
    "# 키워드 매개변수인 languages로 한국어 트윗만 추출합니다\n",
    "stream.sample(languages=['ko'])\n",
    "# stream.sample(languages=['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
